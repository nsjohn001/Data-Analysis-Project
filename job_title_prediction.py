# -*- coding: utf-8 -*-
"""job_title_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BOVJV5vIXxdzDRwLxzQAXs3yqcuegRF9

# Global Fashion Retail Analytics Dataset.

**Upload file**
"""
import pandas as pd

"""**Data Inspection: Begin by loading and exploring the dataset (e.g., checking for missing values, data types, and basic statistics).**

**Access the File in Colab**
"""

import pandas as pd

file_path = "/content/drive/My Drive/Colab Notebooks/customers.csv"
df = pd.read_csv(file_path)
print(df.head())  # Display first few rows

"""**Check for Missing Values**"""

print(df.isnull().sum())  # Shows the number of missing values per column

print(df.isnull().sum().sum())  # Total missing values in the dataset

#visualize missing values using seaborn
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
sns.heatmap(df.isnull(), cmap="viridis", cbar=False, yticklabels=False)
plt.show()

"""**Check Data Types**"""

print(df.dtypes) # Shows the data types per column

"""**Basic Statistics**"""

print(df.describe())  # Shows statistics for numerical columns

print(df.describe(include=['O']))  # Shows statistics for categorical columns

"""**Check Unique Values**"""

#To see the unique values in a column:
print(df['Name'].unique())

print(df['Email'].unique())

print(df['Country'].unique())

#To check how many unique values each column has:
print(df.nunique())

"""**Check for Duplicates**"""

#To check if there are duplicate rows:
print(df.duplicated().sum())

"""**Cleaning the Data: Address missing values, handle outliers, and ensure that the dataset is in a clean format for analysis.**

Cleaning this dataset is a crucial step before analysis.
 Here's how I can handle missing values, detect and remove outliers, and format my data properly.

**Handling Missing Values**
"""

df['Job Title'].fillna('Unknown', inplace=True)

df_cleaned = df.copy()  # Creates a copy of df
print("Missing values after removal:\n", df_cleaned.isnull().sum())

"""**Convert "Date Of Birth" to a proper datetime format.**"""

import pandas as pd

# Convert to datetime
df["Date Of Birth"] = pd.to_datetime(df["Date Of Birth"], errors='coerce')

# Check if conversion was successful
print(df["Date Of Birth"].dtype)  # Should now be datetime64[ns]
print(df["Date Of Birth"].head()) # Verify output

print(df.head())  # Check if dates are formatted correctly

"""**Formatted Telephone Numbers and removed non-numeric characters to ensure consistency.**"""

#Cleaning the "Telephone" Column
import re

df["Telephone"] = df["Telephone"].astype(str).apply(lambda x: re.sub(r'\D', '', x))

print(df["Telephone"].head())  # Displays the first 5 rows

#Checking for duplicate records
duplicates = df[df.duplicated()]
print(duplicates)

"""**Feature Engineering: Create new features or modify existing ones if necessary.**

## Create New Features

**1) Age Calculation from Date of Birth**
"""

from datetime import datetime

# Convert "Date Of Birth" to datetime
df["Date Of Birth"] = pd.to_datetime(df["Date Of Birth"], errors='coerce')

# Calculate "Age"
current_year = datetime.now().year
df["Age"] = current_year - df["Date Of Birth"].dt.year

"""**2) Extract Features from Email**"""

df["Email Username"] = df["Email"].apply(lambda x: x.split("@")[0] if isinstance(x, str) else "")
df["Email Domain"] = df["Email"].apply(lambda x: x.split("@")[1] if isinstance(x, str) else "")

"""**3) Categorize Telephone Number Length**"""

df["Telephone"] = df["Telephone"].astype(str).str.replace(r'\D', '', regex=True)  # Remove non-numeric characters
df["Telephone Length"] = df["Telephone"].apply(len)  # Count number of digits

print(df.head())

"""## Feature Scaling

**I will be using machine learning models, so numerical features like "Age" may need scaling:**
"""

#First, convert "Date Of Birth" to a proper datetime format and calculate "Age"
from datetime import datetime
import pandas as pd

# Convert "Date Of Birth" to datetime format
df["Date Of Birth"] = pd.to_datetime(df["Date Of Birth"], format="%d/%m/%Y", errors='coerce')

# Calculate Age
current_year = datetime.now().year
df["Age"] = current_year - df["Date Of Birth"].dt.year

# Verify Age column
print(df[["Date Of Birth", "Age"]].head())

#Once "Age" is created, I apply MinMax Scaling:
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df["Age_Scaled"] = scaler.fit_transform(df[["Age"]])

# Verify the scaling
print(df[["Age", "Age_Scaled"]].head())

print(df.head())

"""**Handling Text Encoding Issues (Fix non-ASCII characters in names, email, city, and country)**

My dataset contains text encoding issues, where characters appear as gibberish (ÃƒÂ©Ã‚â„¢Ã‚Â¶ÃƒÂ¥Ã‚Â¿Ã‚â€”ÃƒÂ¥Ã‚Â¼Ã‚Âº). This happens when a file is encoded in one format (e.g., UTF-8, Latin-1) but read in another. The code below are solutions to fix this issue.

**Fix Already-Loaded Data**

If some characters remain corrupted, I will remove non-ASCII characters using regex:
"""

import re

# Function to remove non-ASCII characters
def remove_non_ascii(text):
    return re.sub(r'[^\x00-\x7F]+', '', text) if isinstance(text, str) else text

# Apply to relevant columns
for col in ["Name", "Email", "City", "Country"]:
    df[col] = df[col].apply(remove_non_ascii)

# Check the output
print(df.head())

"""**Save and Download the Cleaned Dataset**"""

file_path = "/content/drive/My Drive/Colab Notebooks/cleaned_data1.csv"
df.to_csv(file_path, index=False)
print(f"File saved to: {file_path}")

from google.colab import files

# Save DataFrame as CSV
df.to_csv("cleaned_data2.csv", index=False)

# Download the file
files.download("cleaned_data2.csv")

"""# **Exploratory Data Analysis (EDA) on customers.csv**

**EDA helps understand data patterns, detect anomalies, and find relationships between variables. Below are key steps, along with how visualizations, correlation matrices, and summary statistics will help in analysis.**

**1) Handle Missing Values**
"""

missing_values = df.isnull().sum()
print(missing_values[missing_values > 0])  # Show columns with missing values

"""**Visualization: Bar Chart of Missing Values**"""

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
sns.heatmap(df.isnull(), cbar=False, cmap="viridis")
plt.title("Missing Values Heatmap")
plt.show()

"""**3) Feature Engineering**"""

# Convert "Date Of Birth" into "Age"
from datetime import datetime

df["Date Of Birth"] = pd.to_datetime(df["Date Of Birth"], errors="coerce")
df["Age"] = datetime.now().year - df["Date Of Birth"].dt.year

#Visualization: Age Distribution
sns.histplot(df["Age"], bins=20, kde=True)
plt.title("Age Distribution of Customers")
plt.xlabel("Age")
plt.ylabel("Count")
plt.show()

print(df.head())

"""**4) Categorical Data Analysis**"""

#Top Cities & Countries
print(df["City"].value_counts().head(10))  # Top 10 cities
print(df["Country"].value_counts().head(10))  # Top 10 countries

#Visualization: Top 10 Cities
df["City"].value_counts().head(10).plot(kind="bar", figsize=(10, 5), color="skyblue")
plt.title("Top 10 Cities of Customers")
plt.xlabel("City")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.show()

"""**5) Correlation Analysis**"""

#Find numerical feature relationships:
import numpy as np

correlation_matrix = df.corr(numeric_only=True)
print(correlation_matrix)

# Visualization: Correlation Heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Feature Correlation Heatmap")
plt.show()

"""**6) Gender Distribution**

"""

df["Gender"] = df["Gender"].str.upper()  # Standardize to uppercase

df["Gender"].value_counts().plot(kind="bar", color=["blue", "pink"])
plt.title("Gender Distribution of Customers")
plt.xlabel("Gender")
plt.ylabel("Count")
plt.show()

"""**7) Outlier Detection**"""

#Use Boxplot to find extreme values in "Age":
sns.boxplot(x=df["Age"])
plt.title("Age Outliers")
plt.show()

print(df.head())

"""**Identify any key insights, anomalies, or interesting trends that may guide the modeling process.**

** 1) Age Distribution Insights**
"""

#Summary Statistics of Age
print(df["Age"].describe())

# Group Age into Categories
bins = [0, 18, 30, 45, 60, 100]  # Define age ranges
labels = ["<18", "18-30", "31-45", "46-60", "60+"]
df["Age Group"] = pd.cut(df["Age"], bins=bins, labels=labels)

# Count by Age Group
age_group_counts = df["Age Group"].value_counts().sort_index()
print(age_group_counts)

"""**2) Gender Distribution**"""

#Count Unique Gender Values
print(df["Gender"].value_counts())

#Pie Chart (For Percentage Representation)
plt.figure(figsize=(6, 6))
df["Gender"].value_counts().plot.pie(autopct="%1.1f%%", colors=["skyblue", "pink"], startangle=90)
plt.ylabel("")  # Hide the y-label
plt.title("Gender Distribution")
plt.show()

#Average Age by Gender
print(df.groupby("Gender")["Age"].mean())

# Gender Count by Country/City
print(df.groupby(["Country", "Gender"])["Gender"].count())

"""**3) Job Title Distribution Analysis in my Dataset**"""

#Check Unique Job Titles
print(df["Job Title"].nunique())  # Count of unique job titles
print(df["Job Title"].unique()[:10])  # Show first 10 unique job titles

#Count Job Titles & Handle "Unknown" Values
print(df["Job Title"].value_counts().head(10))  # Top 10 most common job titles

"""**Visualize Job Title Distribution**"""

#Bar Chart (Top 10 Job Titles)
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 5))
top_jobs = df["Job Title"].value_counts().head(10)
sns.barplot(x=top_jobs.values, y=top_jobs.index, palette="coolwarm")
plt.xlabel("Count")
plt.ylabel("Job Title")
plt.title("Top 10 Most Common Job Titles")
plt.show()

"""**4) City & Country Distribution Analysis**"""

# Count Unique Cities & Countries
print("Unique Cities:", df["City"].nunique())
print("Unique Countries:", df["Country"].nunique())

# Most Common Cities & Countries
print(df["City"].value_counts().head(10))  # Top 10 most common cities

# Group Data by Country & City
print(df.groupby("Country")["City"].nunique())  # Unique cities per country
print(df.groupby(["Country", "City"])["City"].count())  # Customer count per city

print(df.head())

"""**5) Fixing Telephone Number Issues in my Dataset**"""

# Convert to String & Remove Scientific Notation
df["Telephone"] = df["Telephone"].astype(str)  # Convert to string
df["Telephone"] = df["Telephone"].apply(lambda x: x.split(".")[0] if "e" in x else x)  # Remove scientific notation

# Standardize Phone Numbers
import re

df["Telephone"] = df["Telephone"].apply(lambda x: re.sub(r"\D", "", x))  # Remove non-numeric characters

# Filter Out Invalid Lengths
df = df[df["Telephone"].str.len().between(10, 15)]

# Remove Duplicates Telephone numbers
df = df.drop_duplicates(subset=["Telephone"])

print(df.head())

"""## **Modeling**

**I will train a Decision Tree Classifier to predict Job Titles**

**First I will Preprocess the Data**
"""

from sklearn.preprocessing import LabelEncoder

print(df["Job Title"].value_counts())

# Remove "Unknown" Job Titles
df_train = df[df["Job Title"] != "Unknown"]
df_test = df[df["Job Title"] == "Unknown"]  # Save for later prediction

# Drop irrelevant columns (Name, Email, Telephone)
df_train = df_train.drop(columns=["Customer ID", "Name", "Email", "Email Username", "Telephone", "Date Of Birth"])

# Encode Categorical Features
from sklearn.preprocessing import LabelEncoder

categorical_cols = ["City", "Country", "Gender", "Email Domain", "Age Group"]
label_encoders = {}

for col in categorical_cols:
    le = LabelEncoder()
    df_train[col] = le.fit_transform(df_train[col].astype(str))  # Convert to string before encoding
    label_encoders[col] = le  # Store encoder for later use

# Group Job Titles into Categories
job_groups = {
    "Engineering": ["Designer, industrial/product", "Scientist, marine"],
    "Healthcare": ["Dentist", "Pathologist"],
    "Creative": ["Special effects artist", "Theme park manager"],
    "Sports": ["Sports therapist"],
    "Management": ["Accommodation manager", "Museum/gallery exhibitions officer"],
}

# Assign job category
df_train["Job Category"] = df_train["Job Title"].map(lambda x: next((k for k, v in job_groups.items() if x in v), "Other"))

# Encode Job Category
job_le = LabelEncoder()
df_train["Job Category"] = job_le.fit_transform(df_train["Job Category"])

"""**Train Decision Tree Model**"""

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Features and Target
X = df_train.drop(columns=["Job Title", "Job Category"])
y = df_train["Job Category"]

# Train-test split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Train Decision Tree model
dt_model = DecisionTreeClassifier(max_depth=8, random_state=42)
dt_model.fit(X_train, y_train)

# Validate model
y_pred = dt_model.predict(X_val)
accuracy = accuracy_score(y_val, y_pred)

print(f"Validation Accuracy: {accuracy:.2f}")

"""**Predict Job Titles for "Unknown" Users**"""

# Predict job category
df_test["Predicted Job Category"] = dt_model.predict(df_test)

# Decode job categories back to names
df_test["Predicted Job Title"] = job_le.inverse_transform(df_test["Predicted Job Category"])

# Show first 10 predictions
print(df_test[["Predicted Job Title"]].head(10))

"""**Train and tune your models using cross-validation and hyperparameter optimization techniques.**

I will train and tune me Decision Tree model using Cross-validation and Hyperparameter optimization with GridSearchCV

**1) Train & Tune Decision Tree with Cross-Validation**
"""

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Filter out rows with 'Unknown' job titles for training
df_train = df[df["Job Title"] != "Unknown"].copy()
df_test = df[df["Job Title"] == "Unknown"].copy()

# Drop Unnecessary Columns
drop_cols = ["Customer ID", "Name", "Email", "Email Username", "Telephone", "Date Of Birth"]
df_train.drop(columns=drop_cols, inplace=True)
df_test.drop(columns=drop_cols + ["Job Title"], inplace=True)

# Encode Categorical Columns
categorical_cols = ["City", "Country", "Gender", "Email Domain", "Age Group", "Job Title"]
label_encoders = {}

for col in categorical_cols:
    le = LabelEncoder()

    if col == "Job Title":
        # Only fit on df_train since df_test has no 'Job Title'
        le.fit(df_train[col].astype(str))
        df_train[col] = le.transform(df_train[col].astype(str))
    else:
        full_data = pd.concat([df_train[col], df_test[col]], axis=0).astype(str)
        le.fit(full_data)
        df_train[col] = le.transform(df_train[col].astype(str))
        df_test[col] = le.transform(df_test[col].astype(str))

    label_encoders[col] = le

# Split Features and Target
X = df_train.drop(columns=["Job Title"])
y = df_train["Job Title"]

"""**Tune Hyperparameters with Cross-Validation**"""

# Set up parameter grid for tuning
param_grid = {
    "max_depth": [10, 15, 20, 25],
    "min_samples_split": [5, 10, 20],
    "min_samples_leaf": [2, 5, 10],
    "criterion": ["gini", "entropy"]
}

# Create model and GridSearchCV
dt = DecisionTreeClassifier(random_state=42)
grid_search = GridSearchCV(dt, param_grid, cv=5, scoring="accuracy", n_jobs=-1)

# Train model
grid_search.fit(X_train, y_train)

# Best parameters
print("Best Parameters:", grid_search.best_params_)

# Evaluate Tuned Model
best_dt = grid_search.best_estimator_
y_pred = best_dt.predict(X_val)
accuracy = accuracy_score(y_val, y_pred)

print(f"Validation Accuracy: {accuracy:.2f}")

# View: Top Rows of the Training Data
print("ðŸ”¹ Sample of Training Data:")
print(df_train.head())

# View: Features and Labels
print("ðŸ”¹ Features (X_train):")
print(X_train.head())

print("\nðŸ”¹ Target Labels (y_train):")
print(y_train.head())

# View: Best Hyperparameters
print("âœ… Best Parameters from Grid Search:")
print(grid_search.best_params_)

# Predict on validation set
y_pred = best_dt.predict(X_val)

# Compare predictions with actual
comparison_df = pd.DataFrame({
    "Actual": y_val,
    "Predicted": y_pred
})
print("ðŸ” Comparison of Actual vs Predicted Job Titles:")
print(comparison_df.head(10))

"""**Evaluate the Decision Tree model performance using appropriate metrics (accuracy, precision, recall, F1 score, RMSE, etc.).**

Evaluating my model using multiple metrics gives a deeper understanding of its performance beyond just accuracy â€” especially in classification problems like predicting Job Titles.        Here's how to evaluate my Decision Tree model:
"""

# Import the Metrics
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, mean_squared_error
import numpy as np

"""**Evaluate Model on Validation Set**"""

# Predict on validation data
y_pred = best_dt.predict(X_val)

# Accuracy
accuracy = accuracy_score(y_val, y_pred)
print(f"âœ… Accuracy: {accuracy:.2f}")

# Precision, Recall, F1 Score
precision = precision_score(y_val, y_pred, average='weighted')
recall = recall_score(y_val, y_pred, average='weighted')
f1 = f1_score(y_val, y_pred, average='weighted')

print(f"ðŸŽ¯ Precision: {precision:.2f}")
print(f"ðŸ“¢ Recall: {recall:.2f}")
print(f"ðŸ“Œ F1 Score: {f1:.2f}")

# RMSE
rmse = np.sqrt(mean_squared_error(y_val, y_pred))
print(f"ðŸ“‰ RMSE (just for info): {rmse:.2f}")

"""**Analyze Feature Importance**"""

import matplotlib.pyplot as plt
import seaborn as sns

# Get feature importances
importances = best_dt.feature_importances_
features = X.columns

# Create a DataFrame
feat_df = pd.DataFrame({"Feature": features, "Importance": importances})
feat_df = feat_df.sort_values("Importance", ascending=False)

# Plot
plt.figure(figsize=(10,6))
sns.barplot(data=feat_df, x="Importance", y="Feature", palette="viridis")
plt.title("ðŸ” Feature Importance from Decision Tree")
plt.xlabel("Importance Score")
plt.ylabel("Feature")
plt.show()
